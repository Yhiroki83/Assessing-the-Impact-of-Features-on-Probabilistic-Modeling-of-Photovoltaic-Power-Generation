{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import properscoring as prscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_gain():\n",
    "    target = [\"generation\"]\n",
    "    features = ['Humidity', 'WindSpeed', 'Temp', 'CloudCover', 'Rain', 'SolarIrradiation', \n",
    "                'yearSin', 'yearCos', 'daySin', 'dayCos','monthSin', 'monthCos', 'hourSin', 'hourCos']\n",
    "    #train = pd.read_csv(\"gs://hiroki_storage/pv/energies/code_energies/train.csv\")\n",
    "    df = pd.read_csv(\"/home/jupyter/code/data_energies.csv\")\n",
    "    train = df[df.month!=6]\n",
    "    train_x,train_y = train[features],train[target]\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=71)\n",
    "    model.fit(train_x, train_y)\n",
    "    feature_gain = model.feature_importances_\n",
    "    feature_gain = pd.DataFrame(feature_gain,index=features,columns=[\"gain\"]).sort_values(ascending=False,by=\"gain\")\n",
    "    return feature_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(day,use_col):\n",
    "    df = pd.read_csv(\"/home/jupyter/code/data_energies.csv\")\n",
    "    train,test = df[df.month!=6],df[df.month==6]\n",
    "    \n",
    "    train = pd.concat([train,test[test.day<day]],axis=0)\n",
    "    test= test[test.day==day]\n",
    "\n",
    "    train_x,test_x=train[use_col],test[use_col]\n",
    "    train_y = np.hstack([train[target],train[target]])\n",
    "    test_y = np.hstack([test[target],test[target]])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "\n",
    "    return train_x,test_x,train_y,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qd_test(y_true, y_pred):\n",
    "    y_true = y_true[:,0]\n",
    "    y_u = y_pred[:,0]\n",
    "    y_l = y_pred[:,1]\n",
    "    \n",
    "    K_HU = tf.maximum(0.,tf.sign(y_u - y_true))\n",
    "    K_HL = tf.maximum(0.,tf.sign(y_true - y_l))\n",
    "    K_H = tf.multiply(K_HU, K_HL)\n",
    "    \n",
    "    K_SU = tf.sigmoid(soften_ * (y_u - y_true))\n",
    "    K_SL = tf.sigmoid(soften_ * (y_true - y_l))\n",
    "    K_S = tf.multiply(K_SU, K_SL)\n",
    "    \n",
    "    MPIW_c = tf.reduce_sum(tf.multiply((y_u - y_l),K_H))/tf.reduce_sum(K_H)\n",
    "    PICP_S = tf.reduce_mean(K_S)    \n",
    "    Loss_S = MPIW_c + lambda_ * n_ / (alpha_*(1-alpha_)) * ((tf.maximum(0.,(1-alpha_) - PICP_S))**2)\n",
    "    \n",
    "    return Loss_S,PICP_S,MPIW_c\n",
    "\n",
    "def qd_objective(y_true, y_pred):\n",
    "    y_true = y_true[:,0]\n",
    "    y_u = y_pred[:,0]\n",
    "    y_l = y_pred[:,1]\n",
    "    \n",
    "    K_HU = tf.maximum(0.,tf.sign(y_u - y_true))\n",
    "    K_HL = tf.maximum(0.,tf.sign(y_true - y_l))\n",
    "    K_H = tf.multiply(K_HU, K_HL)\n",
    "    \n",
    "    K_SU = tf.sigmoid(soften_ * (y_u - y_true))\n",
    "    K_SL = tf.sigmoid(soften_ * (y_true - y_l))\n",
    "    K_S = tf.multiply(K_SU, K_SL)\n",
    "    \n",
    "    MPIW_c = tf.reduce_sum(tf.multiply((y_u - y_l),K_H))/tf.reduce_sum(K_H)\n",
    "    PICP_S = tf.reduce_mean(K_S)\n",
    "    \n",
    "    Loss_S = MPIW_c + lambda_ * n_ / (alpha_*(1-alpha_)) * ((tf.maximum(0.,(1-alpha_) - PICP_S))**2)\n",
    "    \n",
    "    return Loss_S\n",
    "\n",
    "def get_LUBE(number_of_features,LR,BETA):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=number_of_features, activation='relu',\n",
    "                    kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.2)))\n",
    "    model.add(Dense(2, activation='linear',\n",
    "                    kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.3), \n",
    "                    bias_initializer=keras.initializers.Constant(value=[3.,-3.]))) \n",
    "    opt = tf.keras.optimizers.Adam(lr=LR, beta_1=BETA)\n",
    "    model.compile(loss=qd_objective, optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_regression_result(LOWER_ALPHA = 0.025,UPPER_ALPHA = 0.975,SEED = 0,Time = 1,features_counts = 1,\\\n",
    "                               lr=0.05, m_tr=3, m_le=7, n_e=500):\n",
    "    \n",
    "    df = pd.read_csv(\"/home/jupyter/code/data_energies.csv\")\n",
    "\n",
    "    train,test = df[df.month!=6],df[(df.month==6)&(df.day<15)]\n",
    "    all_features = ['SolarIrradiation', 'hourSin', 'hourCos', 'yearCos', 'CloudCover',\n",
    "       'yearSin', 'Humidity', 'Temp', 'monthCos', 'WindSpeed', 'daySin',\n",
    "       'dayCos', 'monthSin', 'Rain']\n",
    "    target = [\"generation\"]  \n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    pred = pd.DataFrame()\n",
    "    for i in range(Time):\n",
    "        features = all_features[:features_counts]\n",
    "        X_train,y_train = train[features],train[target]\n",
    "        X_test,y_test = test[features],test[target]\n",
    "\n",
    "        lower_model = GradientBoostingRegressor(loss=\"quantile\",                   \n",
    "                                                alpha=LOWER_ALPHA,\n",
    "                                                random_state=SEED+100*i,\n",
    "                                                learning_rate=lr,\n",
    "                                                max_depth=m_tr,\n",
    "                                                min_samples_leaf=m_le, \n",
    "                                                n_estimators=n_e,\n",
    "                                                verbose=-1)\n",
    "        \n",
    "        upper_model = GradientBoostingRegressor(loss=\"quantile\",\n",
    "                                                alpha=UPPER_ALPHA,\n",
    "                                                random_state=SEED+100*i,\n",
    "                                                learning_rate=lr,\n",
    "                                                max_depth=m_tr,\n",
    "                                                min_samples_leaf=m_le, \n",
    "                                                n_estimators=n_e,\n",
    "                                                verbose=-1)\n",
    "\n",
    "\n",
    "        lower_model.fit(X_train, y_train)\n",
    "        upper_model.fit(X_train, y_train)\n",
    "        # Record actual values on test set\n",
    "        predictions = pd.DataFrame(y_test)\n",
    "        # Predict\n",
    "        predictions['lower'] = lower_model.predict(X_test)\n",
    "        predictions['upper'] = upper_model.predict(X_test)\n",
    "\n",
    "\n",
    "        predictions[\"PICP\"] = 0\n",
    "        predictions.loc[(predictions.upper>=predictions.generation) & (predictions.generation>=predictions.lower),\"PICP\"] = 1\n",
    "        predictions[\"MPIW\"] = predictions[\"upper\"] - predictions[\"lower\"]\n",
    "        predictions['day'] = test[\"day\"]\n",
    "\n",
    "\n",
    "        predictions[\"mean\"] = (predictions[\"lower\"]+predictions[\"upper\"])/2\n",
    "        predictions[\"std\"] = ((predictions[\"lower\"]+predictions[\"upper\"])/2 - predictions[\"lower\"])/1.96\n",
    "\n",
    "        crps = []\n",
    "        for i in range(predictions.shape[0]):\n",
    "            d = predictions.iloc[i]\n",
    "            C = prscore.crps_gaussian(d[\"generation\"], mu=d[\"mean\"], sig=d[\"std\"])\n",
    "            CRPS = C.mean()\n",
    "            crps.append(CRPS)\n",
    "        predictions[\"crps\"] = crps\n",
    "        predictions[\"features\"] = features_counts\n",
    "        pred = pred.append(predictions)\n",
    "\n",
    "        CRPS,MPIW,PICP,Day = [],[],[],[]\n",
    "        for day,group in predictions.groupby(\"day\"):\n",
    "            m = group[group.PICP>0].MPIW.mean()\n",
    "            p = group.PICP.mean()\n",
    "            c = group.crps.mean()\n",
    "            MPIW.append(m)\n",
    "            PICP.append(p)\n",
    "            CRPS.append(c)\n",
    "            Day.append(day)\n",
    "        Loss = MPIW +  5/(0.05*0.95)*np.where(np.array([0]*14)>np.array([0.95]*14) -np.array(PICP),np.array([0]*14) ,np.array([0.95]*14) -np.array(PICP))\n",
    "\n",
    "        scores = pd.DataFrame(list(zip(Day,MPIW,PICP,CRPS)),columns=[\"day\",\"MPIW\",\"PICP\",\"CRPS\"])\n",
    "        scores[\"features\"] = features_counts\n",
    "        result = result.append(scores)\n",
    "    return pred,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "2022-07-12 02:32:57.947153: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "alpha_ = 0.05\n",
    "soften_ = 160\n",
    "n_lambda = 5\n",
    "EPOCHS = 3000\n",
    "LR = 0.001\n",
    "BETA = 0.01\n",
    "Time = 1\n",
    "SEED = 2021\n",
    "number_of_features = 1\n",
    "target = [\"generation\"]\n",
    "features = ['Humidity', 'WindSpeed', 'Temp', 'CloudCover', 'Rain', 'SolarIrradiation', \n",
    "            'yearSin', 'yearCos', 'daySin', 'dayCos','monthSin', 'monthCos', 'hourSin', 'hourCos']\n",
    "\n",
    "seed_everything(SEED)\n",
    "feature_gain = get_feature_gain()\n",
    "\n",
    "\n",
    "result = pd.DataFrame(columns=[\"number_of_features\",\"day\",\"Loss\",\"PICP\",\"MPIW\"])\n",
    "pred = pd.DataFrame(columns=[\"number_of_features\",\"day\",\"upper\",\"lower\"])\n",
    "for i in range(Time):\n",
    "    for day in range(1,15):\n",
    "        pred_ = pd.DataFrame(columns=[\"number_of_features\",\"day\",\"upper\",\"lower\"])\n",
    "        use_col = feature_gain.index[:number_of_features]\n",
    "        X_train,X_test,y_train,y_test = train_test(day,use_col)\n",
    "\n",
    "        n_= y_train.shape[0]\n",
    "        lambda_ = n_lambda*1/n_ \n",
    "\n",
    "        model = get_LUBE(number_of_features,LR,BETA)\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=n_, verbose=0,  validation_split=0.)\n",
    "        y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "        pred_[[\"upper\",\"lower\"]] = y_pred\n",
    "        pred_[[\"number_of_features\",\"day\"]] = number_of_features,day\n",
    "        pred_[\"Time\"] = i\n",
    "        pred = pd.concat([pred,pred_],axis=0)\n",
    "        Loss_S_,PICP_S_,MPIW_c_ = qd_test(y_test,y_pred)\n",
    "        scores = pd.DataFrame([Loss_S_,PICP_S_,MPIW_c_],index=[\"Loss\",\"PICP\",\"MPIW\"]).T\n",
    "        scores[[\"number_of_features\",\"day\"]] = number_of_features,day\n",
    "        scores[\"Time\"] = i\n",
    "        result = pd.concat([result,scores],axis=0)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
